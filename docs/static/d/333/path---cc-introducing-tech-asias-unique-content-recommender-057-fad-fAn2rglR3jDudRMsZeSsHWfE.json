{"pageContext":{"id":"598883","parent":"__SOURCE__","slug":"introducing-tech-asias-unique-content-recommender","internal":{"contentDigest":"999e481a20ee66028aaf191f9796c950","type":"ContentCoPost","owner":"default-site-plugin"},"children":[],"title":"Introducing Tech in Asia‚Äôs unique content recommender","preview":{"source":"https://cdn.techinasia.com/wp-content/uploads/2018/11/unique-content-feature-image-750x428.png","attachment_meta":{"width":null,"height":null,"sizes":{}}},"createdAt":"2018-11-16T00:05:44","content":"<div id=\"attachment_598892\" class=\"wp-caption alignnone\">\n<img class=\"wp-image-598892 size-large\" src=\"https://cdn.techinasia.com/wp-content/uploads/2018/11/unique-content-feature-image-750x428.png\" alt=\"Our orange recommendation machine\" width=\"750\" height=\"428\"><p class=\"wp-caption-text\">Image credit: Joshua Lim, Susi Susanti</p>\n</div>\n<p>At <em>Tech in Asia</em>, we value our readers. Beyond serving quality content, we‚Äôre always finding ways to improve your reading experience.</p>\n<p>As we scale, we find that it‚Äôs getting harder to serve content tailored to each reader‚Äôs tastes. That led us to one question: <strong>How can we serve you content you didn‚Äôt even know you wanted?</strong></p>\n<p>AI has been the solution to many problems in recent times, and this was no exception. At its core, our content recommender leverages machine learning to serve articles that it predicts you will like based on your reading behavior.</p>\n<p>Let‚Äôs use the analogy of a hypothetical fruit recommender to illustrate what sets <em>Tech in Asia</em>‚Äòs content recommender apart from others.</p>\n<p>Let‚Äôs say you are holding an orange. A fruit recommendation machine scans the fruit that you are holding, and determines its attributes: a fruit that is <em>round</em> in shape, <em>orange</em> in color, has a <em>dimpled</em> texture, weighs <em>140 g</em>, and measures about <em>8 cm</em> in diameter.</p>\n<div id=\"attachment_598891\" class=\"wp-caption alignnone\">\n<img class=\"wp-image-598891 size-full\" src=\"https://cdn.techinasia.com/wp-content/uploads/2018/11/0.png\" alt=\"Fruits party\" width=\"595\" height=\"586\"><p class=\"wp-caption-text\">Image credit: Joshua Lim, Susi Susanti</p>\n</div>\n<p>Instead of making selections that have similar superficial attributes ‚Äì which means that tangerines or mandarin oranges are likely to turn up ‚Äì our machine recognizes the implicit relationships between fruits and then suggests citrus fruits such as pomelos and grapefruits.</p>\n<p>Our method introduces an element of discovery, proposing fruits that are not too similar to help you avoid falling into an echo chamber, but also not too different so that recommended fruits remain pleasing to your taste buds. As an added bonus, fruits that are freshly plucked are favored much more than those that have been on the shelf for some time.</p>\n<h2>How we trained our model</h2>\n<p>We‚Äôre using a <em>content-based recommendation</em> approach, and at the core of it is our topic model.</p>\n<p>A topic model contains definitions of a predefined number of topics. Each topic is a collection of relevance scores of the top 10,000 words in our dataset. So each topic will have a unique set of scores, depending on how relevant each word is to the topic. These words are usually ranked in descending order of relevance so they can be visualized and evaluated for quality.</p>\n<div id=\"attachment_598887\" class=\"wp-caption alignnone\">\n<img class=\"wp-image-598887 size-full\" src=\"https://cdn.techinasia.com/wp-content/uploads/2018/11/6.png\" alt=\"LDA top terms for 8 topics\" width=\"595\" height=\"420\"><p class=\"wp-caption-text\">Image credit: Joshua Lim</p>\n</div>\n<p>In the example above, we can see that the first topic emphasizes words that correspond to venture capital, and the third to mobility, and so on.</p>\n<p>An instance of <em>Latent Dirichlet Allocation</em> (LDA) <a href=\"http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\" target=\"_blank\" rel=\"noopener\">(Blei et al., 2003)</a> with <em>Expectation Maximization</em> <a href=\"https://arxiv.org/pdf/1205.2662\" target=\"_blank\" rel=\"noopener\">(Asuncion et al., 2009)</a>, our topic model has been tweaked specifically within the context of our entire article archive.</p>\n<p>Training an LDA model is an example of <em>Unsupervised Learning</em>, where the machine infers abstract insights from the data without any human input. We can see the difference when we contrast it to typical machine learning methods, which require humans to explicitly tag the data before passing it through the model. In our case, a manual process would entail reading articles, arbitrarily determining the topic composition, saving them, and then repeating the process for every article we‚Äôve ever published.</p>\n<p>While LDA automates a lot of this process, it still requires hints from humans, as do most unsupervised learning models. There are three parameters that we fine-tune to arrive at the right model:</p>\n<ol>\n<li>ùó∏, the number of topics we expect the model to find</li>\n<li>ùõÇ, how likely the document is to contain an even mixture of most topics</li>\n<li>ùõÉ, how likely each topic is to contain an even mixture of most words</li>\n</ol>\n<p>Each combination of ùó∏, ùõÇ and ùõÉ values produces a single model. In order to find the model that best fits our dataset, we perform what‚Äôs known as a <em>Grid Search</em> in which we enumerate all sane combinations of ùó∏, ùõÇ and ùõÉ, train a model for each combination, evaluate the recommendations each generates, and pick the best model.</p>\n<div id=\"attachment_598890\" class=\"wp-caption alignnone\">\n<img class=\"wp-image-598890 size-full\" src=\"https://cdn.techinasia.com/wp-content/uploads/2018/11/1.png\" alt=\"Article to document vector transformation\" width=\"595\" height=\"681\"><p class=\"wp-caption-text\">Image credit: Joshua Lim</p>\n</div>\n<h2>How we generate recommendations</h2>\n<p>We first process the article that you‚Äôre currently reading into a machine-readable form known as the <em>document vector</em>. This vector contains the word counts of every useful word in the document.</p>\n<div id=\"attachment_598888\" class=\"wp-caption alignnone\">\n<img class=\"wp-image-598888 size-full\" src=\"https://cdn.techinasia.com/wp-content/uploads/2018/11/2.png\" alt=\"Document vector to topic vector transformation\" width=\"595\" height=\"406\"><p class=\"wp-caption-text\">Image credit: Joshua Lim</p>\n</div>\n<p>This document vector is then run through our fine-tuned topic model, which transforms them into <em>topic vectors</em>, representing the article‚Äôs topic composition. Let‚Äôs say we train a model of three topics. The topic vector above may represent 10 percent philosophy, 50 percent artificial intelligence, and 40 percent technology, depending on the words that the model deems as important in each topic.</p>\n<div id=\"attachment_598889\" class=\"wp-caption alignnone\">\n<img class=\"wp-image-598889 size-full\" src=\"https://cdn.techinasia.com/wp-content/uploads/2018/11/3.png\" alt=\"Topic vector vs topic vector similarity and relevance\" width=\"595\" height=\"645\"><p class=\"wp-caption-text\">Image credit: Joshua Lim</p>\n</div>\n<p>The topic vector for that article is then compared mathematically with that of every other article we‚Äôve ever published using a similarity metric, and we programmatically pick the best matches based on the following criteria:</p>\n<ul>\n<li>Most relevant, in terms of having the same topic composition as the current article</li>\n<li>Freshest content, prioritizing recent articles as opposed to historical ones</li>\n</ul>\n<h2>How we measure content relevance</h2>\n<p>We define the relevance of two pieces of content to one another by the similarity of their topic composition, weighted by freshness.</p>\n<p><em>Cosine Similarity</em> was used as the similarity metric between topic vectors, as it is a tried-and-tested method of measuring similarity in the natural language processing space.</p>\n<div id=\"attachment_598886\" class=\"wp-caption alignnone\">\n<img class=\"wp-image-598886 size-full\" src=\"https://cdn.techinasia.com/wp-content/uploads/2018/11/5.png\" alt=\"Cosine similarity intuition\" width=\"595\" height=\"440\"><p class=\"wp-caption-text\">Image credit: Joshua Lim</p>\n</div>\n<p>Intuitively, the similarity between two vectors can be quantified by taking the cosine of the angle between them. For example, in identical (overlapping) vectors, the angle between them is 0¬∞ so the cosine value is one. Conversely, for vectors that are completely different, the angle between them is 90¬∞ so the cosine value is zero.</p>\n<div id=\"attachment_598885\" class=\"wp-caption alignnone\">\n<img class=\"wp-image-598885 size-full\" src=\"https://cdn.techinasia.com/wp-content/uploads/2018/11/4.png\" alt=\"Time decay factor vs. Time since publishing\" width=\"595\" height=\"485\"><p class=\"wp-caption-text\">Image credit: Joshua Lim</p>\n</div>\n<p>The similarity score is then scaled by applying a time-decay factor, which gives priority to fresh articles and penalizes older articles non-linearly. That‚Äôs because we observed that articles become less relevant at an increasing rate as time passes.</p>\n<p>The relevance score between two articles, therefore, is the product of their similarity scores and their respective time-decay factors.</p>\n<h2>What‚Äôs next?</h2>\n<p>More exciting improvements to the recommender are in the works even as you read this article. In the recommender‚Äôs next iteration, we intend to let others do the work for you.</p>\n<p>What this means is that by looking at the behavior of readers with taste profiles resembling yours, we can use machine learning to predict and deliver content that we think you‚Äôll like the most.</p>\n<p>Going back to our fruit recommender analogy, the new machine will prioritize fruits that others with a similar purchase history have already picked. This means you‚Äôll spend less time scrolling, and you‚Äôll have more time and choices for reading.</p>\n<p>We‚Äôre excited to roll this out so we can give you the best picks from Asia‚Äôs startup ecosystem. Keep your eyes peeled for our updates!</p>","categories":[{"id":"15462","name":"Announcements","slug":"announcements"}],"author":{"name":"Will Ho","image":"https://cdn.techinasia.com/wp-content/authors/187323.jpg?v=1540280010"},"seo":{"title":"Introducing Tech in Asia‚Äôs unique content recommender","description":"How can we serve you content you didn‚Äôt even know you wanted?","image":"https://cdn.techinasia.com/wp-content/uploads/2018/11/unique-content-feature-image-750x428.png"}}}